# Colab Notebook: Topic Modeling with Fine-Tuned RoBERTa and ICIS Corpus

# %% [markdown]
# # Topic Modeling on ICIS Commodity News
# This notebook demonstrates how to load a private fine-tuned RoBERTa model and dataset from Hugging Face,
# generate embeddings, cluster them, and visualize topics similar to the examples from:
# - Clustering Contextual Embeddings for Topic Model (Towards Data Science)
# - SBERT Topic Modeling Example

# %%
# 1. Install dependencies
!pip install --quiet transformers datasets sentence-transformers huggingface-hub umap-learn scikit-learn plotly

# %%
# 2. Login to Hugging Face Hub (for private repos)
from huggingface_hub import notebook_login
notebook_login()

# %%
# 3. Import libraries
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
import umap
from sklearn.cluster import AgglomerativeClustering
import plotly.express as px

# %%
# 4. Load fine-tuned RoBERTa as a SentenceTransformer
model_id = "Netizine/icis"
embedder = SentenceTransformer(model_id)

# %%
# 5. Load the ICIS dataset
dataset = load_dataset("Netizine/icis")
texts = dataset["train"]["text"]  # adjust split as needed

# %%
# 6. Generate embeddings (batch for large corpora)
batch_size = 32
embeddings = embedder.encode(texts, batch_size=batch_size, show_progress_bar=True)

# %%
# 7. Dimensionality reduction with UMAP
reducer = umap.UMAP(n_neighbors=15, n_components=2, metric='cosine')
emb_2d = reducer.fit_transform(embeddings)

# %%
# 8. Clustering embeddings
n_clusters = 15  # tune as appropriate
clusterer = AgglomerativeClustering(n_clusters=n_clusters)
labels = clusterer.fit_predict(embeddings)

# %%
# 9. Visualization
import pandas as pd

df = pd.DataFrame({
    'x': emb_2d[:,0],
    'y': emb_2d[:,1],
    'cluster': labels,
    'text': texts
})

fig = px.scatter(
    df, x='x', y='y', color='cluster', hover_data=['text'],
    title='Topic Clusters of ICIS Commodity News'
)
fig.update_traces(marker={'size': 5})
fig.show()

# %% [markdown]
# ## Optional: Extract Top Terms per Cluster
# You can vectorize texts with CountVectorizer and print top-n terms for each cluster.

# %%
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(stop_words='english', max_features=10000)
X_counts = vectorizer.fit_transform(texts)
terms = vectorizer.get_feature_names_out()

for cluster_id in range(n_clusters):
    idx = labels == cluster_id
    # sum term frequencies in this cluster
    freqs = X_counts[idx].sum(axis=0).A1
    top_terms = [terms[i] for i in freqs.argsort()[-10:][::-1]]
    print(f"Cluster {cluster_id}: {', '.join(top_terms)}")
